{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŒ´ Asistente TurÃ­stico de Tenerife - RAG Demo\n",
    "\n",
    "## 1. ConfiguraciÃ³n del Entorno\n",
    "Cargamos las librerÃ­as necesarias y verificamos la API Key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Key cargada: sk-pr...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if api_key:\n",
    "    print(f\"âœ… Key cargada: {api_key[:5]}...\")\n",
    "else:\n",
    "    print(\"âŒ Error de Key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "âœ… Dependencias verificadas. Si hubo instalaciones nuevas, ve a Kernel > Restart.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# 0. INSTALACIÃ“N DE DEPENDENCIAS\n",
    "# Ejecuta esto PRIMERO. Si hay actualizaciones, Reinicia el Kernel antes de seguir.\n",
    "%pip install -q -U openai-agents nest_asyncio openai langchain-openai python-dotenv\n",
    "print(\"âœ… Dependencias verificadas. Si hubo instalaciones nuevas, ve a Kernel > Restart.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ValidaciÃ³n de Conectividad con OpenAI\n",
    "Probamos que la llave funcione y el modelo responda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– Respuesta Modelo: Hola Tenerife. Â¿En quÃ© puedo ayudarte hoy?\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Inicializamos el modelo\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "# Prueba 'HolÃ­stica'\n",
    "try:\n",
    "    response = llm.invoke(\"Di 'Hola Tenerife' si me escuchas.\")\n",
    "    print(f\"ðŸ¤– Respuesta Modelo: {response.content}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error de ConexiÃ³n: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fase 2: Ingesta y Vector Store ðŸ“š\n",
    "Subimos el PDF `TENERIFE.pdf` a OpenAI para crear un Vector Store gestionado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ Python Executable: f:\\development\\Development\\Master IA\\LLM-large-language-models-entrega-tarea-final\\llm-env\\Scripts\\python.exe\n",
      "ðŸ“¦ OpenAI Version: 2.15.0\n",
      "âœ… Archivo encontrado: ..\\data\\raw\\TENERIFE.pdf\n",
      "âœ… Conectado vÃ­a client.vector_stores\n",
      "ðŸ“¦ Vector Store creado: vs_6966702747488191acdee4377bb268e6\n",
      "ðŸ“„ Estado de carga: completed\n",
      "ðŸ”¢ Archivos procesados: FileCounts(cancelled=0, completed=1, failed=0, in_progress=0, total=1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. DIAGNÃ“STICO DE ENTORNO (CRÃTICO)\n",
    "print(f\"ðŸ Python Executable: {sys.executable}\")\n",
    "if \"llm-env\" not in sys.executable:\n",
    "    print(\"âš ï¸ ADVERTENCIA: No parece que estÃ©s usando el entorno virtual 'llm-env'. Verifica tu Kernel.\")\n",
    "\n",
    "print(f\"ðŸ“¦ OpenAI Version: {openai.__version__}\")\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "pdf_path = Path(\"../data/raw/TENERIFE.pdf\")\n",
    "\n",
    "if not pdf_path.exists():\n",
    "    print(f\"âŒ No encontrado: {pdf_path}\")\n",
    "else:\n",
    "    print(f\"âœ… Archivo encontrado: {pdf_path}\")\n",
    "\n",
    "    # 2. ADAPTADOR UNIVERSAL DE VECTOR STORES\n",
    "    try:\n",
    "        vs_manager = None\n",
    "        \n",
    "        # Intento 1: Ruta estÃ¡ndar V2 Beta\n",
    "        if hasattr(client, 'beta') and hasattr(client.beta, 'vector_stores'):\n",
    "            vs_manager = client.beta.vector_stores\n",
    "            print(\"âœ… Conectado vÃ­a client.beta.vector_stores\")\n",
    "            \n",
    "        # Intento 2: Ruta raÃ­z (posible cambio futuro o versiÃ³n diferente)\n",
    "        elif hasattr(client, 'vector_stores'):\n",
    "            vs_manager = client.vector_stores\n",
    "            print(\"âœ… Conectado vÃ­a client.vector_stores\")\n",
    "\n",
    "        if vs_manager:\n",
    "            vector_store = vs_manager.create(name=\"Tenerife Guide Store\")\n",
    "            print(f\"ðŸ“¦ Vector Store creado: {vector_store.id}\")\n",
    "            \n",
    "            with open(pdf_path, \"rb\") as f:\n",
    "                file_batch = vs_manager.file_batches.upload_and_poll(\n",
    "                    vector_store_id=vector_store.id, files=[f]\n",
    "                )\n",
    "            print(f\"ðŸ“„ Estado de carga: {file_batch.status}\")\n",
    "            print(f\"ðŸ”¢ Archivos procesados: {file_batch.file_counts}\")\n",
    "        else:\n",
    "            # FALLO TOTAL: Imprimir introspecciÃ³n para depurar\n",
    "            print(\"âŒ ERROR: No se encuentra 'vector_stores' en el cliente OpenAI.\")\n",
    "            print(\"ðŸ” DepuraciÃ³n de atributos disponibles:\")\n",
    "            print(f\"   client dir: {[x for x in dir(client) if 'vector' in x or 'beta' in x]}\")\n",
    "            if hasattr(client, 'beta'):\n",
    "                print(f\"   client.beta dir: {dir(client.beta)}\")\n",
    "            else:\n",
    "                print(\"   client.beta NO existe.\")\n",
    "            raise AttributeError(\"No se pudo acceder a Vector Stores con esta versiÃ³n de librerÃ­a/entorno.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ExcepciÃ³n: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fase 3: ConfiguraciÃ³n del Agente RAG ðŸ¤–\n",
    "Configuramos el agente con capacidad de usar el Vector Store. \n",
    "**Nota**: Incluimos la instalaciÃ³n de librerÃ­as en el kernel para asegurar que el entorno de ejecuciÃ³n tenga todo lo necesario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– Agente 'TenerifeGuide' configurado satisfactoriamente.\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# CorrecciÃ³n: La librerÃ­a se importa como 'agents' aunque se instale como 'openai-agents'\n",
    "from agents import Agent, Runner, FileSearchTool\n",
    "\n",
    "# Recuperamos el ID del Vector Store de la fase anterior\n",
    "# Fallback a un ID conocido si se ha reiniciado el kernel sin re-ejecutar la Fase 2\n",
    "try:\n",
    "    current_vs_id = vector_store.id\n",
    "except NameError:\n",
    "    current_vs_id = \"vs_69661c161c5881919eadf77744958c70\" # ID de ejemplo/previo\n",
    "    print(f\"âš ï¸ Usando ID recuperado/hardcodeado: {current_vs_id}\")\n",
    "\n",
    "# 1. Definir la herramienta de bÃºsqueda de archivos\n",
    "file_search_tool = FileSearchTool(vector_store_ids=[current_vs_id])\n",
    "\n",
    "# 2. Instrucciones del Agente (System Prompt)\n",
    "INSTRUCTIONS = \"\"\"\n",
    "Eres un experto guÃ­a turÃ­stico de Tenerife.\n",
    "Usa la herramienta file_search para encontrar informaciÃ³n en la guÃ­a PDF adjunta.\n",
    "Si la informaciÃ³n no estÃ¡ en el PDF, dilo claramente. No inventes respuestas.\n",
    "Responde siempre en espaÃ±ol, con un tono amable y entusiasta.\n",
    "\"\"\"\n",
    "\n",
    "# 3. Crear el Agente\n",
    "agent = Agent(\n",
    "    name=\"TenerifeGuide\",\n",
    "    model=\"gpt-4o-mini\",\n",
    "    instructions=INSTRUCTIONS,\n",
    "    tools=[file_search_tool]\n",
    ")\n",
    "\n",
    "print(f\"ðŸ¤– Agente '{agent.name}' configurado satisfactoriamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prueba RÃ¡pida del Agente\n",
    "Lanzamos una pregunta de prueba para verificar que el RAG funciona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘¤ Usuario: Â¿QuÃ© actividades puedo hacer en el Teide?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error getting response: Error code: 400 - {'error': {'message': \"Tool 'file_search' is not supported with gpt-3.5-turbo.\", 'type': 'invalid_request_error', 'param': 'tools', 'code': None}}. (request_id: req_707e24a3adb544079cc94c78f159b1eb)\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"Tool 'file_search' is not supported with gpt-3.5-turbo.\", 'type': 'invalid_request_error', 'param': 'tools', 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m     resultado = \u001b[38;5;28;01mawait\u001b[39;00m Runner.run(agent, pregunta)\n\u001b[32m      7\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mðŸ¤– Agente: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresultado.final_response\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m demo_agente()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mdemo_agente\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mðŸ‘¤ Usuario: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpregunta\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Ejecutamos el agente\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m resultado = \u001b[38;5;28;01mawait\u001b[39;00m Runner.run(agent, pregunta)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mðŸ¤– Agente: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresultado.final_response\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\development\\Development\\Master IA\\LLM-large-language-models-entrega-tarea-final\\llm-env\\Lib\\site-packages\\agents\\run.py:370\u001b[39m, in \u001b[36mRunner.run\u001b[39m\u001b[34m(cls, starting_agent, input, context, max_turns, hooks, run_config, previous_response_id, auto_previous_response_id, conversation_id, session)\u001b[39m\n\u001b[32m    321\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    322\u001b[39m \u001b[33;03mRun a workflow starting at the given agent.\u001b[39;00m\n\u001b[32m    323\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    366\u001b[39m \u001b[33;03m    type of the output.\u001b[39;00m\n\u001b[32m    367\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    369\u001b[39m runner = DEFAULT_AGENT_RUNNER\n\u001b[32m--> \u001b[39m\u001b[32m370\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m runner.run(\n\u001b[32m    371\u001b[39m     starting_agent,\n\u001b[32m    372\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    373\u001b[39m     context=context,\n\u001b[32m    374\u001b[39m     max_turns=max_turns,\n\u001b[32m    375\u001b[39m     hooks=hooks,\n\u001b[32m    376\u001b[39m     run_config=run_config,\n\u001b[32m    377\u001b[39m     previous_response_id=previous_response_id,\n\u001b[32m    378\u001b[39m     auto_previous_response_id=auto_previous_response_id,\n\u001b[32m    379\u001b[39m     conversation_id=conversation_id,\n\u001b[32m    380\u001b[39m     session=session,\n\u001b[32m    381\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\development\\Development\\Master IA\\LLM-large-language-models-entrega-tarea-final\\llm-env\\Lib\\site-packages\\agents\\run.py:668\u001b[39m, in \u001b[36mAgentRunner.run\u001b[39m\u001b[34m(self, starting_agent, input, **kwargs)\u001b[39m\n\u001b[32m    660\u001b[39m     sequential_results = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._run_input_guardrails(\n\u001b[32m    661\u001b[39m         starting_agent,\n\u001b[32m    662\u001b[39m         sequential_guardrails,\n\u001b[32m    663\u001b[39m         _copy_str_or_list(prepared_input),\n\u001b[32m    664\u001b[39m         context_wrapper,\n\u001b[32m    665\u001b[39m     )\n\u001b[32m    667\u001b[39m \u001b[38;5;66;03m# Run parallel guardrails + agent together.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m668\u001b[39m input_guardrail_results, turn_result = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m    669\u001b[39m     \u001b[38;5;28mself\u001b[39m._run_input_guardrails(\n\u001b[32m    670\u001b[39m         starting_agent,\n\u001b[32m    671\u001b[39m         parallel_guardrails,\n\u001b[32m    672\u001b[39m         _copy_str_or_list(prepared_input),\n\u001b[32m    673\u001b[39m         context_wrapper,\n\u001b[32m    674\u001b[39m     ),\n\u001b[32m    675\u001b[39m     \u001b[38;5;28mself\u001b[39m._run_single_turn(\n\u001b[32m    676\u001b[39m         agent=current_agent,\n\u001b[32m    677\u001b[39m         all_tools=all_tools,\n\u001b[32m    678\u001b[39m         original_input=original_input,\n\u001b[32m    679\u001b[39m         generated_items=generated_items,\n\u001b[32m    680\u001b[39m         hooks=hooks,\n\u001b[32m    681\u001b[39m         context_wrapper=context_wrapper,\n\u001b[32m    682\u001b[39m         run_config=run_config,\n\u001b[32m    683\u001b[39m         should_run_agent_start_hooks=should_run_agent_start_hooks,\n\u001b[32m    684\u001b[39m         tool_use_tracker=tool_use_tracker,\n\u001b[32m    685\u001b[39m         server_conversation_tracker=server_conversation_tracker,\n\u001b[32m    686\u001b[39m     ),\n\u001b[32m    687\u001b[39m )\n\u001b[32m    689\u001b[39m \u001b[38;5;66;03m# Combine sequential and parallel results.\u001b[39;00m\n\u001b[32m    690\u001b[39m input_guardrail_results = sequential_results + input_guardrail_results\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python313\\Lib\\asyncio\\tasks.py:375\u001b[39m, in \u001b[36mTask.__wakeup\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m    373\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[32m    374\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m375\u001b[39m         \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    376\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    377\u001b[39m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[32m    378\u001b[39m         \u001b[38;5;28mself\u001b[39m.__step(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python313\\Lib\\asyncio\\tasks.py:304\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    300\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    301\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    302\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    303\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    306\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\development\\Development\\Master IA\\LLM-large-language-models-entrega-tarea-final\\llm-env\\Lib\\site-packages\\agents\\run.py:1631\u001b[39m, in \u001b[36mAgentRunner._run_single_turn\u001b[39m\u001b[34m(cls, agent, all_tools, original_input, generated_items, hooks, context_wrapper, run_config, should_run_agent_start_hooks, tool_use_tracker, server_conversation_tracker)\u001b[39m\n\u001b[32m   1628\u001b[39m     \u001b[38;5;28minput\u001b[39m = ItemHelpers.input_to_new_input_list(original_input)\n\u001b[32m   1629\u001b[39m     \u001b[38;5;28minput\u001b[39m.extend([generated_item.to_input_item() \u001b[38;5;28;01mfor\u001b[39;00m generated_item \u001b[38;5;129;01min\u001b[39;00m generated_items])\n\u001b[32m-> \u001b[39m\u001b[32m1631\u001b[39m new_response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._get_new_response(\n\u001b[32m   1632\u001b[39m     agent,\n\u001b[32m   1633\u001b[39m     system_prompt,\n\u001b[32m   1634\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   1635\u001b[39m     output_schema,\n\u001b[32m   1636\u001b[39m     all_tools,\n\u001b[32m   1637\u001b[39m     handoffs,\n\u001b[32m   1638\u001b[39m     hooks,\n\u001b[32m   1639\u001b[39m     context_wrapper,\n\u001b[32m   1640\u001b[39m     run_config,\n\u001b[32m   1641\u001b[39m     tool_use_tracker,\n\u001b[32m   1642\u001b[39m     server_conversation_tracker,\n\u001b[32m   1643\u001b[39m     prompt_config,\n\u001b[32m   1644\u001b[39m )\n\u001b[32m   1646\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._get_single_step_result_from_response(\n\u001b[32m   1647\u001b[39m     agent=agent,\n\u001b[32m   1648\u001b[39m     original_input=original_input,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1657\u001b[39m     tool_use_tracker=tool_use_tracker,\n\u001b[32m   1658\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\development\\Development\\Master IA\\LLM-large-language-models-entrega-tarea-final\\llm-env\\Lib\\site-packages\\agents\\run.py:1889\u001b[39m, in \u001b[36mAgentRunner._get_new_response\u001b[39m\u001b[34m(cls, agent, system_prompt, input, output_schema, all_tools, handoffs, hooks, context_wrapper, run_config, tool_use_tracker, server_conversation_tracker, prompt_config)\u001b[39m\n\u001b[32m   1879\u001b[39m previous_response_id = (\n\u001b[32m   1880\u001b[39m     server_conversation_tracker.previous_response_id\n\u001b[32m   1881\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m server_conversation_tracker\n\u001b[32m   1882\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m server_conversation_tracker.previous_response_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1883\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1884\u001b[39m )\n\u001b[32m   1885\u001b[39m conversation_id = (\n\u001b[32m   1886\u001b[39m     server_conversation_tracker.conversation_id \u001b[38;5;28;01mif\u001b[39;00m server_conversation_tracker \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1887\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1889\u001b[39m new_response = \u001b[38;5;28;01mawait\u001b[39;00m model.get_response(\n\u001b[32m   1890\u001b[39m     system_instructions=filtered.instructions,\n\u001b[32m   1891\u001b[39m     \u001b[38;5;28minput\u001b[39m=filtered.input,\n\u001b[32m   1892\u001b[39m     model_settings=model_settings,\n\u001b[32m   1893\u001b[39m     tools=all_tools,\n\u001b[32m   1894\u001b[39m     output_schema=output_schema,\n\u001b[32m   1895\u001b[39m     handoffs=handoffs,\n\u001b[32m   1896\u001b[39m     tracing=get_model_tracing_impl(\n\u001b[32m   1897\u001b[39m         run_config.tracing_disabled, run_config.trace_include_sensitive_data\n\u001b[32m   1898\u001b[39m     ),\n\u001b[32m   1899\u001b[39m     previous_response_id=previous_response_id,\n\u001b[32m   1900\u001b[39m     conversation_id=conversation_id,\n\u001b[32m   1901\u001b[39m     prompt=prompt_config,\n\u001b[32m   1902\u001b[39m )\n\u001b[32m   1904\u001b[39m context_wrapper.usage.add(new_response.usage)\n\u001b[32m   1906\u001b[39m \u001b[38;5;66;03m# If we have run hooks, or if the agent has hooks, we need to call them after the LLM call\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\development\\Development\\Master IA\\LLM-large-language-models-entrega-tarea-final\\llm-env\\Lib\\site-packages\\agents\\models\\openai_responses.py:97\u001b[39m, in \u001b[36mOpenAIResponsesModel.get_response\u001b[39m\u001b[34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, tracing, previous_response_id, conversation_id, prompt)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m response_span(disabled=tracing.is_disabled()) \u001b[38;5;28;01mas\u001b[39;00m span_response:\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m         response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fetch_response(\n\u001b[32m     98\u001b[39m             system_instructions,\n\u001b[32m     99\u001b[39m             \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    100\u001b[39m             model_settings,\n\u001b[32m    101\u001b[39m             tools,\n\u001b[32m    102\u001b[39m             output_schema,\n\u001b[32m    103\u001b[39m             handoffs,\n\u001b[32m    104\u001b[39m             previous_response_id=previous_response_id,\n\u001b[32m    105\u001b[39m             conversation_id=conversation_id,\n\u001b[32m    106\u001b[39m             stream=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    107\u001b[39m             prompt=prompt,\n\u001b[32m    108\u001b[39m         )\n\u001b[32m    110\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m _debug.DONT_LOG_MODEL_DATA:\n\u001b[32m    111\u001b[39m             logger.debug(\u001b[33m\"\u001b[39m\u001b[33mLLM responded\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\development\\Development\\Master IA\\LLM-large-language-models-entrega-tarea-final\\llm-env\\Lib\\site-packages\\agents\\models\\openai_responses.py:320\u001b[39m, in \u001b[36mOpenAIResponsesModel._fetch_response\u001b[39m\u001b[34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, previous_response_id, conversation_id, stream, prompt)\u001b[39m\n\u001b[32m    316\u001b[39m         response_format = {\u001b[33m\"\u001b[39m\u001b[33mverbosity\u001b[39m\u001b[33m\"\u001b[39m: model_settings.verbosity}\n\u001b[32m    318\u001b[39m stream_param: Literal[\u001b[38;5;28;01mTrue\u001b[39;00m] | Omit = \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m omit\n\u001b[32m--> \u001b[39m\u001b[32m320\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.responses.create(\n\u001b[32m    321\u001b[39m     previous_response_id=\u001b[38;5;28mself\u001b[39m._non_null_or_omit(previous_response_id),\n\u001b[32m    322\u001b[39m     conversation=\u001b[38;5;28mself\u001b[39m._non_null_or_omit(conversation_id),\n\u001b[32m    323\u001b[39m     instructions=\u001b[38;5;28mself\u001b[39m._non_null_or_omit(system_instructions),\n\u001b[32m    324\u001b[39m     model=model_param,\n\u001b[32m    325\u001b[39m     \u001b[38;5;28minput\u001b[39m=list_input,\n\u001b[32m    326\u001b[39m     include=include,\n\u001b[32m    327\u001b[39m     tools=tools_param,\n\u001b[32m    328\u001b[39m     prompt=\u001b[38;5;28mself\u001b[39m._non_null_or_omit(prompt),\n\u001b[32m    329\u001b[39m     temperature=\u001b[38;5;28mself\u001b[39m._non_null_or_omit(model_settings.temperature),\n\u001b[32m    330\u001b[39m     top_p=\u001b[38;5;28mself\u001b[39m._non_null_or_omit(model_settings.top_p),\n\u001b[32m    331\u001b[39m     truncation=\u001b[38;5;28mself\u001b[39m._non_null_or_omit(model_settings.truncation),\n\u001b[32m    332\u001b[39m     max_output_tokens=\u001b[38;5;28mself\u001b[39m._non_null_or_omit(model_settings.max_tokens),\n\u001b[32m    333\u001b[39m     tool_choice=tool_choice,\n\u001b[32m    334\u001b[39m     parallel_tool_calls=parallel_tool_calls,\n\u001b[32m    335\u001b[39m     stream=cast(Any, stream_param),\n\u001b[32m    336\u001b[39m     extra_headers=\u001b[38;5;28mself\u001b[39m._merge_headers(model_settings),\n\u001b[32m    337\u001b[39m     extra_query=model_settings.extra_query,\n\u001b[32m    338\u001b[39m     extra_body=model_settings.extra_body,\n\u001b[32m    339\u001b[39m     text=response_format,\n\u001b[32m    340\u001b[39m     store=\u001b[38;5;28mself\u001b[39m._non_null_or_omit(model_settings.store),\n\u001b[32m    341\u001b[39m     prompt_cache_retention=\u001b[38;5;28mself\u001b[39m._non_null_or_omit(model_settings.prompt_cache_retention),\n\u001b[32m    342\u001b[39m     reasoning=\u001b[38;5;28mself\u001b[39m._non_null_or_omit(model_settings.reasoning),\n\u001b[32m    343\u001b[39m     metadata=\u001b[38;5;28mself\u001b[39m._non_null_or_omit(model_settings.metadata),\n\u001b[32m    344\u001b[39m     **extra_args,\n\u001b[32m    345\u001b[39m )\n\u001b[32m    346\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(Union[Response, AsyncStream[ResponseStreamEvent]], response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\development\\Development\\Master IA\\LLM-large-language-models-entrega-tarea-final\\llm-env\\Lib\\site-packages\\openai\\resources\\responses\\responses.py:2480\u001b[39m, in \u001b[36mAsyncResponses.create\u001b[39m\u001b[34m(self, background, conversation, include, input, instructions, max_output_tokens, max_tool_calls, metadata, model, parallel_tool_calls, previous_response_id, prompt, prompt_cache_key, prompt_cache_retention, reasoning, safety_identifier, service_tier, store, stream, stream_options, temperature, text, tool_choice, tools, top_logprobs, top_p, truncation, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   2442\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   2443\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   2444\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2478\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m   2479\u001b[39m ) -> Response | AsyncStream[ResponseStreamEvent]:\n\u001b[32m-> \u001b[39m\u001b[32m2480\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   2481\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m/responses\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2482\u001b[39m         body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m   2483\u001b[39m             {\n\u001b[32m   2484\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mbackground\u001b[39m\u001b[33m\"\u001b[39m: background,\n\u001b[32m   2485\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mconversation\u001b[39m\u001b[33m\"\u001b[39m: conversation,\n\u001b[32m   2486\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33minclude\u001b[39m\u001b[33m\"\u001b[39m: include,\n\u001b[32m   2487\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33minput\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   2488\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33minstructions\u001b[39m\u001b[33m\"\u001b[39m: instructions,\n\u001b[32m   2489\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_output_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_output_tokens,\n\u001b[32m   2490\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: max_tool_calls,\n\u001b[32m   2491\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   2492\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   2493\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   2494\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprevious_response_id\u001b[39m\u001b[33m\"\u001b[39m: previous_response_id,\n\u001b[32m   2495\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m: prompt,\n\u001b[32m   2496\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprompt_cache_key\u001b[39m\u001b[33m\"\u001b[39m: prompt_cache_key,\n\u001b[32m   2497\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprompt_cache_retention\u001b[39m\u001b[33m\"\u001b[39m: prompt_cache_retention,\n\u001b[32m   2498\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mreasoning\u001b[39m\u001b[33m\"\u001b[39m: reasoning,\n\u001b[32m   2499\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33msafety_identifier\u001b[39m\u001b[33m\"\u001b[39m: safety_identifier,\n\u001b[32m   2500\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   2501\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   2502\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   2503\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m: stream_options,\n\u001b[32m   2504\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   2505\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m: text,\n\u001b[32m   2506\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   2507\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   2508\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m   2509\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   2510\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtruncation\u001b[39m\u001b[33m\"\u001b[39m: truncation,\n\u001b[32m   2511\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   2512\u001b[39m             },\n\u001b[32m   2513\u001b[39m             response_create_params.ResponseCreateParamsStreaming\n\u001b[32m   2514\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[32m   2515\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m response_create_params.ResponseCreateParamsNonStreaming,\n\u001b[32m   2516\u001b[39m         ),\n\u001b[32m   2517\u001b[39m         options=make_request_options(\n\u001b[32m   2518\u001b[39m             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   2519\u001b[39m         ),\n\u001b[32m   2520\u001b[39m         cast_to=Response,\n\u001b[32m   2521\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2522\u001b[39m         stream_cls=AsyncStream[ResponseStreamEvent],\n\u001b[32m   2523\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\development\\Development\\Master IA\\LLM-large-language-models-entrega-tarea-final\\llm-env\\Lib\\site-packages\\openai\\_base_client.py:1797\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1784\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1785\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1792\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1793\u001b[39m ) -> ResponseT | _AsyncStreamT:\n\u001b[32m   1794\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1795\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1796\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1797\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\development\\Development\\Master IA\\LLM-large-language-models-entrega-tarea-final\\llm-env\\Lib\\site-packages\\openai\\_base_client.py:1597\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1594\u001b[39m             \u001b[38;5;28;01mawait\u001b[39;00m err.response.aread()\n\u001b[32m   1596\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1597\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1599\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1601\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'error': {'message': \"Tool 'file_search' is not supported with gpt-3.5-turbo.\", 'type': 'invalid_request_error', 'param': 'tools', 'code': None}}"
     ]
    }
   ],
   "source": [
    "async def demo_agente():\n",
    "    pregunta = \"Â¿QuÃ© actividades puedo hacer en el Teide?\"\n",
    "    print(f\"ðŸ‘¤ Usuario: {pregunta}\")\n",
    "    \n",
    "    # Ejecutamos el agente\n",
    "    resultado = await Runner.run(agent, pregunta)\n",
    "    print(f\"ðŸ¤– Agente: {resultado.final_response}\")\n",
    "\n",
    "await demo_agente()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}